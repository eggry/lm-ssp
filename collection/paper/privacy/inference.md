# C5. Inference
- [2024/03] **[CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following](https://arxiv.org/abs/2403.03129)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2402.17840)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/02] **[The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2402.16893)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/02] **[Pandora's White-Box: Increased Training Data Leakage in Open LLMs](https://arxiv.org/abs/2402.17012)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Large Language Models are Advanced Anonymizers](https://arxiv.org/abs/2402.13846)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Privacy-Preserving Language Model Inference with Instance Obfuscation](https://arxiv.org/abs/2402.08227)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/02] **[Do Membership Inference Attacks Work on Large Language Models?](https://arxiv.org/abs/2402.07841)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models](https://arxiv.org/abs/2402.05868)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/agiresearch/PromptCrypt) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/01] **[Excuse me, sir? Your language model is leaking (information)](https://arxiv.org/abs/2401.10360)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[R-Judge: Benchmarking Safety Risk Awareness for LLM Agents](https://arxiv.org/abs/2401.10019)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2023/12] **[Black-box Membership Inference Attacks against Fine-tuned Diffusion Models](https://arxiv.org/abs/2312.08207)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2023/11] **[Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://arxiv.org/abs/2311.06062)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[User Inference Attacks on Large Language Models](https://arxiv.org/abs/2310.09266)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning](https://arxiv.org/abs/2310.11397)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization](https://openreview.net/forum?id=rpH9FcCEV6)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Beyond Memorization: Violating Privacy via Inference with Large Language Models](https://openreview.net/forum?id=kmn0BhQk7p)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory](https://openreview.net/forum?id=c93SBwz1Ma)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Identifying the Risks of LM Agents with an LM-Emulated Sandbox](https://openreview.net/forum?id=GEcwtMk1uA)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Privacy Side Channels in Machine Learning Systems](https://arxiv.org/abs/2309.05610)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/08] **[White-box Membership Inference Attacks against Diffusion Models](https://arxiv.org/abs/2308.06405)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2023/07] **[ProPILE: Probing Privacy Leakage in Large Language Models](https://arxiv.org/abs/2307.01881)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'23](https://img.shields.io/badge/NeurIPS'23-f1b800)
- [2023/03] **[Class Attribute Inference Attacks: Inferring Sensitive Class Information by Diffusion-Based Attribute Manipulations](https://arxiv.org/abs/2303.09289)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2022/10] **[Membership Inference Attacks Against Text-to-image Generation Models](https://arxiv.org/abs/2210.00968)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
