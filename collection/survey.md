# Survey
- [2024/02] **[Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[A Survey of Text Watermarking in the Era of Large Language Models](https://arxiv.org/abs/2312.07913)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Safety of Multimodal Large Language Models on Images and Text ](https://arxiv.org//abs/2402.00357)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/02] **[A Survey on Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2402.00253)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/01] **[Security and Privacy Challenges of Large Language Models: A Survey](https://arxiv.org/abs/2402.00888)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Black-Box Access is Insufficient for Rigorous AI Audits](https://arxiv.org/abs/2401.14446)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Red Teaming Visual Language Models](https://arxiv.org/abs/2401.12915)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/01] **[Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems](https://arxiv.org/abs/2401.05778)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[TrustLLM: Trustworthiness in Large Language Models](https://arxiv.org/abs/2401.05561)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/HowieHwong/TrustLLM) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[Privacy Issues in Large Language Models: A Survey](https://arxiv.org/abs/2312.06717)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly](https://arxiv.org/abs/2312.02003)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks](https://arxiv.org/abs/2310.10844)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[AgentBench: Evaluating LLMs as Agents](https://openreview.net/forum?id=zAdUB0aCTQ)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/08] **[Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment](https://arxiv.org/abs/2308.05374)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/07] **[A Comprehensive Overview of Large Language Models](https://arxiv.org/abs/2307.06435)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/06] **[DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models](https://arxiv.org/abs/2306.11698)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://decodingtrust.github.io/) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'23](https://img.shields.io/badge/NeurIPS'23-f1b800) ![Best Paper](https://img.shields.io/badge/Best_paper-ff0000)
- [2023/05] **[ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review](https://arxiv.org/abs/2305.03123)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/04] **[Safety Assessment of Chinese Large Language Models](https://arxiv.org/abs/2304.10436)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/03] **[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/11] **[Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![TMLR'23](https://img.shields.io/badge/TMLR'23-f1b800)
- [2022/08] **[Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/06] **[Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://arxiv.org/abs/2206.04615)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2021/11] **[Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models](https://arxiv.org/abs/2111.02840)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'21](https://img.shields.io/badge/NeurIPS'21-f1b800)
